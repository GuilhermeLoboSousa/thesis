{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c6e9b83-1785-420a-96d3-8f76d1bd393c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__ProPythia__\n",
    "\n",
    "This file is a simulation for the antioxidant dataset;\n",
    "\n",
    "I intend to run propythia here so that in the future I can make comparisons with the results obtained with omnia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d1af9c7-f66b-4bec-9b91-314233424118",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "i will do this simulation with the antioxidant data, where the data in unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "413683bb-3e9e-4835-a9e7-81a88d10fcdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:31:54.648011Z",
     "start_time": "2024-05-16T14:31:54.621253Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "638d8edb-1125-4f3b-a21a-6c7ad23e5203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from propythia.protein.descriptors import ProteinDescritors\n",
    "from propythia.protein.encoding import Encoding\n",
    "from propythia.ml.shallow_ml import ShallowML\n",
    "from propythia.protein.sequence import ReadSequence\n",
    "\n",
    "from propythia.ml.deep_ml import DeepML\n",
    "\n",
    "from propythia.feature_selection import FeatureSelection\n",
    "\n",
    "from propythia.preprocess import Preprocess\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fac73e2-9117-4d24-9ff5-0c1630992edd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# read the data is the first step !! do not forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd544c28-4121-41d8-8a3e-22f4fa173514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_seqs = ReadSequence()\n",
    "x_train_nlf = read_seqs.par_preprocessing(dataset= x_train_nlf, col = 'sequence', B ='N', Z = 'Q', U = 'C', O = 'K', J = 'I', X = '')\n",
    "x_test_nlf = read_seqs.par_preprocessing(dataset= x_test_nlf, col = 'sequence', B ='N', Z = 'Q', U = 'C', O = 'K', J = 'I', X = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac6e0c66-cf26-48db-8fee-a1e8a8ed7eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Encoding\n",
    "\n",
    "First create a function that allows to padding and truncate the sequence , i choose the value X like the character to be add in case of padding;\n",
    "\n",
    "\n",
    "next aplicate the encoding nlf classe from propythia\n",
    "\n",
    "__Note__: 600 is a high value for our dataset, but is the value more mentioned in literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T13:26:12.913241Z",
     "start_time": "2024-05-16T13:26:12.902806Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f42c6f77-41e0-4965-988c-09ae1b5cf113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pad_and_truncate_sequences(df, seq_col, max_length, padding_value='X'):\n",
    "    \"\"\"\n",
    "    Pad and truncate the protein sequences in a DataFrame to a specific length.\n",
    "\n",
    "    :param df: DataFrame containing the protein sequences.\n",
    "    :param seq_col: Name of the column in df that contains the protein sequences.\n",
    "    :param max_length: The maximum length for all sequences.\n",
    "    :param padding_value: The value to use for padding the sequences.\n",
    "    :return: DataFrame with the padded and truncated sequences.\n",
    "    \"\"\"\n",
    "    def pad_and_truncate(seq):\n",
    "        # Truncate the sequence if it's too long\n",
    "        if len(seq) > max_length:\n",
    "            seq = seq[:max_length]\n",
    "        # Pad the sequence if it's not long enough\n",
    "        elif len(seq) < max_length:\n",
    "            seq += padding_value * (max_length - len(seq))\n",
    "        return seq\n",
    "\n",
    "    df['padded_and_truncated_sequence'] = df[seq_col].apply(pad_and_truncate)\n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "x_train_nlf_encode = pad_and_truncate_sequences(x_train_nlf, 'sequence', 600)\n",
    "x_test_nlf_encode = pad_and_truncate_sequences(x_test_nlf, 'sequence', 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T13:31:09.886039Z",
     "start_time": "2024-05-16T13:31:09.867665Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356655c1-81f5-4baa-a387-ae55f54c7bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enconde_train_df = Encoding(dataset= x_train_nlf_encode ,  col= 'padded_and_truncated_sequence')\n",
    "enconde_test_df=Encoding(dataset= x_test_nlf_encode ,  col= 'padded_and_truncated_sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T13:31:10.587778Z",
     "start_time": "2024-05-16T13:31:10.482678Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d038d09c-ca2f-47fc-8b22-71baffc9cd61",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train\n",
    "nlf_train=enconde_train_df.get_nlf()\n",
    "nlf_test=enconde_test_df.get_nlf()\n",
    "expanded_train_arrays =  nlf_train['nlf'].apply(lambda x: np.array(x))\n",
    "#test\n",
    "x_train_nlf_ = np.array(expanded_train_arrays.tolist())\n",
    "expanded_test_arrays =  nlf_test['nlf'].apply(lambda x: np.array(x))\n",
    "x_test_nlf_ = np.array(expanded_test_arrays.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a9aa398-8f7c-4942-b721-b0cce6a203a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x_test_nlf_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375b1493-30ed-4bc2-8d69-6096c856ccad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# split antioxidant_dataset X and Y \n",
    "\n",
    "the goal is to split the datatese in train , test and do the normalization for the x descriptors\n",
    "\n",
    "We decide to use the same dataset for test and train for all the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:03:06.163258Z",
     "start_time": "2024-05-16T14:03:04.119670Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f16e8b-ddeb-44e9-9622-b26bbd8bf019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#X_train_nlf, X_test_nlf, y_train_nlf, y_test_nlf = train_test_split(X_nlf,y_nlf, test_size=0.2,stratify=y_nlf, random_state=42)\n",
    "\n",
    "# standard scaler article does not refer scaling and do not validate in x_test, however, we do it anyway\n",
    "#scaler = StandardScaler().fit(X_train_descriptors)\n",
    "#X_train_descriptors = scaler.transform(X_train_descriptors)\n",
    "#X_test_descriptors = scaler.transform(X_test_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3eea65f-84e7-4d1e-85e5-9f0c6a01c0b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "The aim is to recreate the deepLearning models implemented in omnia as well as the possibility of optimising hyperparameters by grid search (using a param_grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5b59f9-3482-4be5-885f-4711f9ec2c61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T10:01:43.679202Z",
     "start_time": "2024-05-20T10:01:43.435481Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6573102-a7cf-4c13-81c6-e3bce5bc91bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RNN and propythia\n",
    "def create_rnn_model(rnn_type='LSTM', bidirectional=False, num_rnn_layers=1, hidden_dim=64, num_dense_layers=1, neurons_dense=32, output_dim=1, drop=0.3, activation='relu', last_layers_activations='sigmoid'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # RNN layers\n",
    "    for i in range(num_rnn_layers):\n",
    "        current_hidden_dim =hidden_dim // (2**i)\n",
    "        if rnn_type == 'LSTM':\n",
    "            rnn_layer = tf.keras.layers.LSTM(current_hidden_dim, return_sequences=(i != num_rnn_layers - 1), activation=activation)\n",
    "        elif rnn_type == 'GRU':\n",
    "            rnn_layer = tf.keras.layers.GRU(current_hidden_dim, return_sequences=(i != num_rnn_layers - 1), activation=activation)\n",
    "        elif rnn_type == 'SimpleRNN':\n",
    "            rnn_layer = tf.keras.layers.SimpleRNN(current_hidden_dim, return_sequences=(i != num_rnn_layers - 1), activation=activation)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Supported types are 'LSTM', 'GRU', and 'SimpleRNN'.\")\n",
    "\n",
    "        if bidirectional:\n",
    "            rnn_layer = tf.keras.layers.Bidirectional(rnn_layer)\n",
    "\n",
    "        model.add(rnn_layer)\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    # Dense layers\n",
    "    for i in range(num_dense_layers):\n",
    "        current_neurons_dense = neurons_dense if i == 0 else neurons_dense // (2**i)\n",
    "        model.add(tf.keras.layers.Dense(current_neurons_dense, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(output_dim, activation=last_layers_activations))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dl=DeepML(x_train_nlf, y_train_nlf, x_test_nlf, y_test_nlf, number_classes=2, problem_type='binary',\n",
    "          x_dval=None, y_dval=None, epochs=10, batch_size=32,\n",
    "          path='', report_name=None, verbose=1,\n",
    "         early_stopping_patience=2, reduce_lr_patience=10, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                 )\n",
    "model = KerasClassifier(build_fn=create_rnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe35be5-0de7-4ead-827d-e6c0ff56743d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%capture captured\n",
    "\n",
    "def generate_param_grid(num_rnn_layers:list, num_dense_layers:list):\n",
    "    param_grid = {\n",
    "        'rnn_type': ['LSTM', 'GRU', 'SimpleRNN'],\n",
    "        'bidirectional': [True, False],\n",
    "        'num_rnn_layers': num_rnn_layers,\n",
    "        'hidden_dim': [64,128],\n",
    "        'num_dense_layers': num_dense_layers,\n",
    "        'neurons_dense': [64,128],\n",
    "        'output_dim': [1],\n",
    "        'drop': [0.1, 0.3],\n",
    "        'activation': ['relu'],\n",
    "        'last_layers_activations': ['sigmoid']\n",
    "    }\n",
    "    return param_grid\n",
    "\n",
    "# Define os parâmetros do grid\n",
    "num_rnn_layers = [2,3]\n",
    "num_dense_layers = [2,3]\n",
    "param_grid = generate_param_grid(num_rnn_layers, num_dense_layers)\n",
    "\n",
    "# Otimização do modelo\n",
    "best_classifier_rnn = dl.get_opt_params(param_grid, model, scoring=make_scorer(f1_score), optType='randomizedSearch', cv=5, n_iter_search=50)\n",
    "\n",
    "# Avaliação do modelo\n",
    "scores, report, cm, cm2 = dl.score_testset_classification(best_classifier_rnn)\n",
    "\n",
    "# Predição dos valores para o teste\n",
    "y_pred = best_classifier_rnn.predict(dl.x_test)\n",
    "\n",
    "# Matriz de confusão\n",
    "conf_mat = confusion_matrix(dl.y_test, y_pred)\n",
    "\n",
    "# Guarda o output capturado num ficheiro\n",
    "with open('output_data_nlf_rnn.log', 'w') as f:\n",
    "    f.write(str(captured))\n",
    "\n",
    "# Guarda os scores num ficheiro .txt\n",
    "with open('scores_data_nlf_rnn.txt', 'w') as f:\n",
    "    f.write(report)  # O relatório normalmente já contém f1-score, precisão, recall\n",
    "\n",
    "# Guarda a confusion matrix num ficheiro .txt ou .csv\n",
    "np.savetxt('confusion_matrix_data_nlf_rnn.log.csv', conf_mat, delimiter=\",\")\n",
    "\n",
    "# Visualiza a matriz de confusão\n",
    "dl.conf_matrix_seaborn_table(conf_matrix=conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "NLF_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
