{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c27d877a-7b06-4ee6-80c7-ed934d8accaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__ProPythia__\n",
    "\n",
    "This file is a simulation for the antioxidant dataset;\n",
    "\n",
    "I intend to run propythia here so that in the future I can make comparisons with the results obtained with omnia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b0b2b5-e5a0-4c19-a3c3-3319da159b73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "i will do this simulation with the antioxidant data, where the data in unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682f3d01-425f-4e0c-9eac-a8c77a40689d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:31:54.648011Z",
     "start_time": "2024-05-16T14:31:54.621253Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6d4026-95ec-41a0-a719-09236c596bc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from propythia.protein.descriptors import ProteinDescritors\n",
    "from propythia.protein.encoding import Encoding\n",
    "from propythia.ml.shallow_ml import ShallowML\n",
    "from propythia.protein.sequence import ReadSequence\n",
    "\n",
    "from propythia.ml.deep_ml import DeepML\n",
    "\n",
    "from propythia.feature_selection import FeatureSelection\n",
    "\n",
    "from propythia.preprocess import Preprocess\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f80206b1-cae9-44c8-941d-70588a976544",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Encoding\n",
    "\n",
    "__Note__: 600 is a high value for our dataset, but is the value more mentioned in literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5878bc-b462-41ac-b8e5-ad9a06a6fc69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# read the data is the first step !! do not forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e00db241-40b3-4ee8-ae10-78677327f14d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_seqs = ReadSequence()\n",
    "x_train_esm = read_seqs.par_preprocessing(dataset= x_train_esm, col = 'sequence', B ='N', Z = 'Q', U = 'C', O = 'K', J = 'I', X = '')\n",
    "x_test_esm = read_seqs.par_preprocessing(dataset= x_test_esm, col = 'sequence', B ='N', Z = 'Q', U = 'C', O = 'K', J = 'I', X = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd16791-6731-43f1-b654-208d3f1c8497",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pad_and_truncate_sequences(df,seq_col,max_length) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    It pads or truncates the sequences to the maximum sequence length.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: pd.DataFrame\n",
    "        The input data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: pd.DataFrame\n",
    "        The padded or truncated input data.\n",
    "    \"\"\"\n",
    "\n",
    "    def pad_and_truncate(seq):\n",
    "        # Truncate the sequence if it's longer than max_seq_len\n",
    "        if len(seq) > max_length:\n",
    "            return seq[:max_length]\n",
    "        # Pad the sequence if it's shorter than max_seq_len\n",
    "        else:\n",
    "            padding = \"<pad>\" * (max_length - len(seq))\n",
    "            return seq + padding\n",
    "    df['padded_and_truncated_sequence'] = df[seq_col].apply(pad_and_truncate)\n",
    "    return df\n",
    "\n",
    "x_train_esm_encode = pad_and_truncate_sequences(x_train_esm, 'sequence', 600)\n",
    "x_test_esm_encode = pad_and_truncate_sequences(x_test_esm, 'sequence', 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c8ff04a-39b5-4389-8479-a9f250b067d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enconde_train_df = Encoding(dataset= x_train_esm_encode ,  col= 'padded_and_truncated_sequence')\n",
    "enconde_test_df=Encoding(dataset= x_test_esm_encode ,  col= 'padded_and_truncated_sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T13:31:09.886039Z",
     "start_time": "2024-05-16T13:31:09.867665Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7febb07b-690d-4851-9a07-60c6bb084890",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "esm_train = enconde_train_df.get_esm(v_esm='esm2_150')\n",
    "x_train_esm=np.array([x.astype(np.float64) for x in esm_train['esm']])\n",
    "\n",
    "esm_test = enconde_test_df.get_esm(v_esm='esm2_150')\n",
    "x_test_esm=np.array([x.astype(np.float64) for x in esm_test['esm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8f4e952-3228-42f6-94b7-6d641d14aab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_train_esm=y_train_esm.drop(3274)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d98b9532-78f0-46fd-9801-d223e9e8af29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "The aim is to recreate the deepLearning models implemented in omnia as well as the possibility of optimising hyperparameters by grid search (using a param_grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2e1c4ff-086b-4d47-ada0-97577a64ae87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T10:01:43.679202Z",
     "start_time": "2024-05-20T10:01:43.435481Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a75d4f36-b42d-46b0-998c-3b3bf960ded9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RNN and propythia\n",
    "def create_rnn_model(rnn_type='LSTM', bidirectional=False, num_rnn_layers=1, hidden_dim=64, num_dense_layers=1, neurons_dense=32, output_dim=1, drop=0.3, activation='relu', last_layers_activations='sigmoid'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # RNN layers\n",
    "    for i in range(num_rnn_layers):\n",
    "        current_hidden_dim =hidden_dim // (2**i)\n",
    "        if rnn_type == 'LSTM':\n",
    "            rnn_layer = tf.keras.layers.LSTM(current_hidden_dim, return_sequences=(i != num_rnn_layers - 1), activation=activation)\n",
    "        elif rnn_type == 'GRU':\n",
    "            rnn_layer = tf.keras.layers.GRU(current_hidden_dim, return_sequences=(i != num_rnn_layers - 1), activation=activation)\n",
    "        elif rnn_type == 'SimpleRNN':\n",
    "            rnn_layer = tf.keras.layers.SimpleRNN(current_hidden_dim, return_sequences=(i != num_rnn_layers - 1), activation=activation)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN type. Supported types are 'LSTM', 'GRU', and 'SimpleRNN'.\")\n",
    "\n",
    "        if bidirectional:\n",
    "            rnn_layer = tf.keras.layers.Bidirectional(rnn_layer)\n",
    "\n",
    "        model.add(rnn_layer)\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    # Dense layers\n",
    "    for i in range(num_dense_layers):\n",
    "        current_neurons_dense = neurons_dense if i == 0 else neurons_dense // (2**i)\n",
    "        model.add(tf.keras.layers.Dense(current_neurons_dense, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(output_dim, activation=last_layers_activations))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dl=DeepML(x_train_esm, y_train_esm, x_test_esm, y_test_esm, number_classes=2, problem_type='binary',\n",
    "          x_dval=None, y_dval=None, epochs=80, batch_size=32,\n",
    "          path='', report_name=None, verbose=1,\n",
    "         early_stopping_patience=15, reduce_lr_patience=10, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                 )\n",
    "model = KerasClassifier(build_fn=create_rnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T10:01:45.376744Z",
     "start_time": "2024-05-20T10:01:45.349746Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83327ddd-6310-456a-a697-2c138b933b50",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture captured\n",
    "\n",
    "def generate_param_grid(num_rnn_layers:list, num_dense_layers:list):\n",
    "    param_grid = {\n",
    "        'rnn_type': ['LSTM', 'GRU', 'SimpleRNN'],\n",
    "        'bidirectional': [True, False],\n",
    "        'num_rnn_layers': num_rnn_layers,\n",
    "        'hidden_dim': [64,128],\n",
    "        'num_dense_layers': num_dense_layers,\n",
    "        'neurons_dense': [64,128],\n",
    "        'output_dim': [1],\n",
    "        'drop': [0.1, 0.3],\n",
    "        'activation': ['relu'],\n",
    "        'last_layers_activations': ['sigmoid']\n",
    "    }\n",
    "    return param_grid\n",
    "\n",
    "# Define os parâmetros do grid\n",
    "num_rnn_layers = [2,3]\n",
    "num_dense_layers = [2,3]\n",
    "param_grid = generate_param_grid(num_rnn_layers, num_dense_layers)\n",
    "\n",
    "# Otimização do modelo\n",
    "best_classifier_rnn = dl.get_opt_params(param_grid, model, scoring=make_scorer(f1_score), optType='randomizedSearch', cv=5, n_iter_search=50)\n",
    "dl.save_model(path='best_rnn_model_esm.h5', model=best_classifier_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51463c6f-9100-4d7b-a391-d6e53b91d524",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores, report, cm, cm2 = dl.score_testset_classification(best_classifier_rnn)\n",
    "\n",
    "# Escreva cada métrica e valor em uma linha separada\n",
    "with open('scores_data_esm_rnn.txt', 'w') as f:\n",
    "    for metric, value in scores.items():\n",
    "        f.write(f\"{metric}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7f385e-d8c4-466a-b3f7-e9f43f46b8d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred = best_classifier_rnn.predict(dl.x_test)\n",
    "\n",
    "conf_mat = confusion_matrix(dl.y_test, y_pred)\n",
    "np.savetxt('confusion_matrix__data_esm2_rnn.csv', conf_mat, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "ESM_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
